<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Workshop Content | Integrating Theory and Data in Plant-Pollinator Interactions</title>
    <link>https://models4data2theory.github.io/courses/</link>
      <atom:link href="https://models4data2theory.github.io/courses/index.xml" rel="self" type="application/rss+xml" />
    <description>Workshop Content</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 14 Aug 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://models4data2theory.github.io/media/icon_hu0409958f157fc50d4e355b357a277bb5_195869_512x512_fill_lanczos_center_3.png</url>
      <title>Workshop Content</title>
      <link>https://models4data2theory.github.io/courses/</link>
    </image>
    
    <item>
      <title>Prework - R and RStudio</title>
      <link>https://models4data2theory.github.io/courses/prework_01_install_r/</link>
      <pubDate>Thu, 15 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://models4data2theory.github.io/courses/prework_01_install_r/</guid>
      <description>&lt;p&gt;&lt;em&gt;Before you arrive at RMBL.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Tutorial on setting up R and Rstudio.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This is a tutorial on how to install the R language and RStudio development environment.
It was modified from &lt;a href=&#34;https://github.com/rstudio/learnr/tree/main/inst/tutorials/ex-setup-r/ex-setup-r.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The tutorial will help you set up your computer to use R. It is for you if you need to learn how to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install R on your computer&lt;/li&gt;
&lt;li&gt;Install the RStudio IDE&lt;/li&gt;
&lt;li&gt;Install packages, using the &lt;code&gt;dplyr&lt;/code&gt; R package as an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can skip this tutorial if you&amp;rsquo;ve already done these things.&lt;/p&gt;
&lt;h3 id=&#34;is-this-tutorial-for-you&#34;&gt;Is this tutorial for you?&lt;/h3&gt;
&lt;p&gt;Do you need to work through the tutorial? Take the quiz below to find out.&lt;/p&gt;
&lt;p&gt;::: {.webex-check .webex-box}
I have installed R on my computer &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;
I have installed the RStudio IDE &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;
I have installed the dplyr R package &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;If you have done all of these things, you can skip this tutorial and go to the next one!
:::&lt;/p&gt;
&lt;h2 id=&#34;install-r&#34;&gt;Install R&lt;/h2&gt;
&lt;h3 id=&#34;how-to-install-r&#34;&gt;How to install R&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://vimeo.com/203516510&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Follow the video instructions here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;test-your-knowledge&#34;&gt;Test your knowledge&lt;/h3&gt;
&lt;p&gt;::: {.webex-check .webex-box}
Is R free to download and use? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;Where do you download R? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;&lt;a href=&#34;https://www.rstudio.com/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.rstudio.com/download&lt;/a&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;cloud.r-project.org&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;&lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.r-project.org&lt;/a&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;&lt;a href=&#34;https://www.r.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.r.com&lt;/a&gt;&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;How often should you update R? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;Everytime you use it&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;About once a year&lt;/option&gt;&lt;option value=&#39;&#39;&gt;Never&lt;/option&gt;&lt;/select&gt;
:::&lt;/p&gt;
&lt;h2 id=&#34;install-rstudio&#34;&gt;Install RStudio&lt;/h2&gt;
&lt;h3 id=&#34;how-to-install-rstudio&#34;&gt;How to install RStudio&lt;/h3&gt;
&lt;p&gt;RStudio is an Integrated Development Environment for R. What does that mean? Well, if you think of R as a language, which it is, you can think of RStudio as a program that helps you write and work in the language. RStudio makes programming in R much easier and I suggest that you use it!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://vimeo.com/203516968&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Follow the video instructions here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;test-your-knowledge-1&#34;&gt;Test your knowledge&lt;/h3&gt;
&lt;p&gt;::: {.webex-check .webex-box}
What is the RStudio IDE? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;An application that makes it easier to use R.&lt;/option&gt;&lt;option value=&#39;&#39;&gt;An application that let&amp;rsquo;s you use R without writing any code&lt;/option&gt;&lt;option value=&#39;&#39;&gt;A spreadsheet program like Microsoft Excel.&lt;/option&gt;&lt;option value=&#39;&#39;&gt;Another name for R&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;Is the RStudio IDE free to download and use? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;Where do you download RStudio? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;&lt;a href=&#34;https://www.rstudio.com/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.rstudio.com/download&lt;/a&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;cloud.r-project.org&lt;/option&gt;&lt;option value=&#39;&#39;&gt;&lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.r-project.org&lt;/a&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;cran.rstudio.org&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;Do you need to install R if you already have RStudio? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;
:::&lt;/p&gt;
&lt;h2 id=&#34;install-packages&#34;&gt;Install Packages&lt;/h2&gt;
&lt;h3 id=&#34;how-to-install-r-packages&#34;&gt;How to install R packages&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://vimeo.com/203516241&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Follow the video instructions here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;test-your-knowledge-2&#34;&gt;Test your knowledge&lt;/h3&gt;
&lt;p&gt;::: {.webex-check .webex-box}
What command do you use to install packages? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;library()&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;install.packages()&lt;/option&gt;&lt;option value=&#39;&#39;&gt;There is no command. You must visit &lt;a href=&#34;http://cran.r-project.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cran.r-project.org&lt;/a&gt; and download packages manually.&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;How often do you need to install a package on your computer? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;Every time you restart R&lt;/option&gt;&lt;option value=&#39;&#39;&gt;Every time you restart your computer&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;Only once. Afterwards, R can find it on your hard drive as needed.&lt;/option&gt;&lt;option value=&#39;&#39;&gt;Never, as long as you are connected to the internet.&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;What command do you use to load a package into your R session? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;package()&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;library()&lt;/option&gt;&lt;option value=&#39;&#39;&gt;install.packages()&lt;/option&gt;&lt;/select&gt;
:::&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Prework - R Data Basics</title>
      <link>https://models4data2theory.github.io/courses/prework_02_data_basics/</link>
      <pubDate>Thu, 15 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://models4data2theory.github.io/courses/prework_02_data_basics/</guid>
      <description>&lt;p&gt;&lt;em&gt;Before you arrive at RMBL.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Intro to data structures, help pages, and variable types in R.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this tutorial, you will learn how to use R to inspect the contents of a data frame or tibble.
Data frames and tibbles are R&amp;rsquo;s structures for storing tabular data;
if you inherit a tabular dataset in R, it will almost certainly come as one of these structures.&lt;/p&gt;
&lt;p&gt;Here, you will learn how to do three things with data frames and tibbles:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Look at the contents of a data frame or tibble&lt;/li&gt;
&lt;li&gt;Open a help page that describes a data frame or tibble&lt;/li&gt;
&lt;li&gt;Identify the variables and their types in a tibble&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You will also meet the &lt;code&gt;palmerpenguins&lt;/code&gt; and &lt;code&gt;nycflights&lt;/code&gt; datasets. These datasets appear frequently in R examples.&lt;/p&gt;
&lt;p&gt;The readings in this tutorial follow &lt;a href=&#34;http://r4ds.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;R for Data Science&lt;/em&gt;&lt;/a&gt;, sections 3.2 and 5.1.&lt;/p&gt;
&lt;h2 id=&#34;data-frames&#34;&gt;Data frames&lt;/h2&gt;
&lt;h3 id=&#34;what-is-a-data-frame&#34;&gt;What is a data frame?&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;data frame&lt;/strong&gt; is a rectangular collection of values, usually organized so that variables appear in the columns and observations appear in rows.&lt;/p&gt;
&lt;p&gt;Here is an example: the &lt;code&gt;penguins&lt;/code&gt; data frame contains observations collected and published by &lt;a href=&#34;https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Kristen Gorman&lt;/a&gt; from &lt;a href=&#34;https://pallter.marine.rutgers.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Palmer Station&lt;/a&gt;, a long-term environmental research site in Antarctica. The data frame contains 344 rows and 8 columns. Each row represents a penguin, and each column represents a variable that describes the penguin. Each penguin is one of three different species.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;images/lter_penguins.png&#34; alt=&#34;The three different penguin species in the `palmerpenguins` datasets (credit: Allison Horst)&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Typing &lt;code&gt;penguins&lt;/code&gt; into the &lt;code&gt;R&lt;/code&gt; console prints the header of the &lt;code&gt;penguins&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## # A tibble: 344 × 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;int&amp;gt;       &amp;lt;int&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ℹ 334 more rows
## # ℹ 2 more variables: sex &amp;lt;fct&amp;gt;, year &amp;lt;int&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;a-note-about-palmerpenguins&#34;&gt;A note about palmerpenguins&lt;/h3&gt;
&lt;p&gt;The code above worked because I&amp;rsquo;ve already loaded the &lt;code&gt;palmerpenguins&lt;/code&gt; package in this tutorial: &lt;code&gt;penguins&lt;/code&gt; comes in the &lt;code&gt;palmerpenguins&lt;/code&gt; package. If you would like to look at &lt;code&gt;penguins&lt;/code&gt; on your own computer, you will need to first load &lt;code&gt;palmerpenguins&lt;/code&gt;. You can do that in two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;install.packages(&#39;palmerpenguins&#39;)&lt;/code&gt; to install &lt;code&gt;palmerpenguins&lt;/code&gt; if you do not yet have it.&lt;/li&gt;
&lt;li&gt;Load the package with the &lt;code&gt;library(palmerpenguins)&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Run the command &lt;code&gt;data(package = &#39;palmerpenguins&#39;)&lt;/code&gt; to load the &lt;code&gt;penguins&lt;/code&gt; data frame into your R session.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After that, you will be able to access any dataset contained in the &lt;code&gt;palmerpenguins&lt;/code&gt; package&amp;mdash;until you close R.&lt;/p&gt;
&lt;h3 id=&#34;one-thing-to-notice&#34;&gt;One thing to notice&lt;/h3&gt;
&lt;p&gt;Did you notice how much information was inside &lt;code&gt;penguins&lt;/code&gt;? Me too. Sometimes the contents of a data frame are hard to interpret. Let&amp;rsquo;s get some help with this&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;help-pages&#34;&gt;Help pages&lt;/h2&gt;
&lt;h3 id=&#34;how-to-open-a-help-page&#34;&gt;How to open a help page&lt;/h3&gt;
&lt;p&gt;You can learn more about &lt;code&gt;penguins&lt;/code&gt; by opening its help page. The help page will explain where the &lt;code&gt;palmerpenguins&lt;/code&gt; dataset comes from and what each variable in the &lt;code&gt;penguins&lt;/code&gt; data frame describes. To open the help page, type &lt;code&gt;?penguins&lt;/code&gt; in the code chunk below and then click &amp;ldquo;Run Code&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;the--syntax&#34;&gt;The ? syntax&lt;/h3&gt;
&lt;p&gt;You can open a help page for any object that comes with R or with an R package. To open the help page, type a &lt;code&gt;?&lt;/code&gt; before the object&amp;rsquo;s name and then run the command, as you did with &lt;code&gt;?penguins&lt;/code&gt;. This technique works for functions, packages, and more. If you want to specify getting help for a function or dataset in a particular package, you can use the &lt;code&gt;::&lt;/code&gt; operator. For example, &lt;code&gt;?dplyr::filter&lt;/code&gt; will open the help page for the &lt;code&gt;filter()&lt;/code&gt; function in the &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;Notice that objects created by you or your colleagues will not have a help page (unless you make one).&lt;/p&gt;
&lt;h3 id=&#34;exercises&#34;&gt;Exercises&lt;/h3&gt;
&lt;p&gt;Please answer the following questions.&lt;/p&gt;
&lt;p&gt;::: {.webex-check .webex-box}
What does the &lt;code&gt;bill_depth_mm&lt;/code&gt; variable of &lt;code&gt;penguins&lt;/code&gt; describe?  Read the help for &lt;code&gt;?penguins&lt;/code&gt; to find out. &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;The depth below the surface that the penguin dives to catch fish&lt;/option&gt;&lt;option value=&#39;&#39;&gt;The species of penguin&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;The distance across the bill from the chin to the top of the bill&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;How many rows are in the data frame named &lt;code&gt;penguins&lt;/code&gt;? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;34&lt;/option&gt;&lt;option value=&#39;&#39;&gt;300&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;344&lt;/option&gt;&lt;option value=&#39;&#39;&gt;3344&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;
&lt;p&gt;How many columns are in the data frame named &lt;code&gt;penguins&lt;/code&gt;? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;&#39;&gt;34&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;8&lt;/option&gt;&lt;option value=&#39;&#39;&gt;0&lt;/option&gt;&lt;option value=&#39;&#39;&gt;10&lt;/option&gt;&lt;/select&gt;
:::&lt;/p&gt;
&lt;h2 id=&#34;data-types&#34;&gt;Data types&lt;/h2&gt;
&lt;h3 id=&#34;type-codes&#34;&gt;Type codes&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s return to the &lt;code&gt;penguins&lt;/code&gt; data frame. Run the code chunk below to see the first few rows of &lt;code&gt;penguins&lt;/code&gt; again.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;penguins&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## # A tibble: 344 × 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;int&amp;gt;       &amp;lt;int&amp;gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ℹ 334 more rows
## # ℹ 2 more variables: sex &amp;lt;fct&amp;gt;, year &amp;lt;int&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Did you notice that a row of three (or four) letter abbreviations appears under the column names of &lt;code&gt;penguins&lt;/code&gt;? These abbreviations describe the &lt;em&gt;type&lt;/em&gt; of data that is stored in each column of &lt;code&gt;penguins&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;int&lt;/code&gt; stands for integers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;dbl&lt;/code&gt; stands for doubles, or real numbers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;chr&lt;/code&gt; stands for character vectors, or strings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;dttm&lt;/code&gt; stands for date-times (a date + a time).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are three other common types of variables that aren&amp;rsquo;t used in this dataset but are used in other datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;lgl&lt;/code&gt; stands for logical, vectors that contain only &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fctr&lt;/code&gt; stands for factors, which R uses to represent categorical variables with fixed possible values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;date&lt;/code&gt; stands for dates.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This row of data types is unique to tibbles and is one of the ways that tibbles try to be more user-friendly than data frames.&lt;/p&gt;
&lt;h3 id=&#34;test-your-knowledge&#34;&gt;Test your knowledge&lt;/h3&gt;
&lt;p&gt;::: {.webex-check .webex-box}
Which types of variables does &lt;code&gt;penguins&lt;/code&gt; contain?
Integers? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;
Doubles? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;
Factors? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;
Characters? &lt;select class=&#39;webex-select&#39;&gt;&lt;option value=&#39;blank&#39;&gt;&lt;/option&gt;&lt;option value=&#39;answer&#39;&gt;TRUE&lt;/option&gt;&lt;option value=&#39;&#39;&gt;FALSE&lt;/option&gt;&lt;/select&gt;
:::&lt;/p&gt;
&lt;h2 id=&#34;congratulations&#34;&gt;Congratulations&lt;/h2&gt;
&lt;p&gt;You&amp;rsquo;ve met R&amp;rsquo;s basic table structures and you have learned how to inspect their contents. When you are ready, go on to the next tutorial.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Interactions</title>
      <link>https://models4data2theory.github.io/courses/wkshp_interactions/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://models4data2theory.github.io/courses/wkshp_interactions/</guid>
      <description>&lt;p&gt;&lt;em&gt;Fitting models to data&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$
\newcommand{\L}{\mathcal{L}}
$$&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;A powerful approach to theory-data integration is the use of statistical methods that directly combine empirical data with the mechanistic models of theoretical ecology.
This is the world of model-fitting.&lt;/p&gt;
&lt;p&gt;Perhaps we performed a study in which we counted the number of pollinator visits the flowers of a focal plant species received over a range of different plant densities and now wish to characterize the relationship between these variables.
(To keep things simple, we&amp;rsquo;ll assume each plant has only a single flower.)
Or perhaps we performed an experiment in which we varied the number of pollinator visits that the flowers of the plant species received and are now interested in characterizing how this variation in visits influenced ovule fertilization success (i.e. the proportion of ovules in each flower that were successfully fertilized).&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/motivation-1.png&#34; width=&#34;768&#34; /&gt;
&lt;p&gt;In the past, the common way to characterize any such relationship among variables was to use linear or non-linear least-squares regression (including polynomial regression), extending more recently to mixed effects models.
Generalized linear models and generalized additive models have also become popular, largely because they can accommodate non-Gaussian error structures and flexible, non-linear relationships.&lt;/p&gt;
&lt;p&gt;But these types of statistical models are generally not &amp;ldquo;mechanistic&amp;rdquo;;
their functional forms are not derived from &amp;ldquo;first-principles&amp;rdquo; ecological theory.
Instead, these types of models are by and large only &amp;ldquo;descriptive&amp;rdquo; in nature.&lt;/p&gt;
&lt;p&gt;Theoreticians, on the other hand, have derived many non-statistical (deterministic) mathematical equations to encapsulate how different biological processes should/could influence the patterns we see in nature.
Our goal is to develop the skills to (1) fit such &lt;em&gt;mechanistic models&lt;/em&gt; to data in order to obtain &lt;em&gt;best-fitting parameter estimates&lt;/em&gt; while accommodating &lt;em&gt;process-appropriate error structures&lt;/em&gt;, and (2) compare the &lt;em&gt;relative performance&lt;/em&gt; of several such models in order to identify those that &lt;em&gt;perform best&lt;/em&gt; at representing the data.&lt;/p&gt;
&lt;p&gt;We will achieve these things using the principle of &lt;strong&gt;maximum likelihood&lt;/strong&gt; and an &lt;strong&gt;information-theoretic&lt;/strong&gt; model-comparison approach.
Other approaches for model fitting and comparison &amp;mdash; such as Bayesian statistics &amp;mdash; are often used to accommodate complex data structures and for other pragmatic and epistomological reasons, but those are beyond what we will cover.
That said, most of the principles that we will cover are directly relevant to these other approaches as well.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;To understand the principle of maximum likelihood, we first need to understand some fundamentals regarding &lt;a href=&#34;#prob_dist&#34;&gt;probability distributions&lt;/a&gt; and &lt;a href=&#34;#lik_fun&#34;&gt;likelihood functions&lt;/a&gt;.
We&amp;rsquo;ll then see how theory models can be incorporated into likelihood functions to convert them from deterministic models to statistical models.
It&amp;rsquo;s this conversion that permits us to fit them to data.
We will then develop our intuition for &lt;a href=&#34;#max_lik&#34;&gt;maximum likelihood parameter estimation&lt;/a&gt; (i.e. model fitting) by first doing it &lt;a href=&#34;#max_lik_math&#34;&gt;analytically&lt;/a&gt; and then learning to use &lt;a href=&#34;#max_lik_optim&#34;&gt;numerical optimization&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Having fit several models to a dataset, we will then dip our toes into the methods of comparing their relative performance using &lt;a href=&#34;#mod_comp&#34;&gt;information criteria&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Skip down to the &lt;a href=&#34;#end_result&#34;&gt;end result&lt;/a&gt; to see where we&amp;rsquo;re headed.&lt;/p&gt;
&lt;h4 id=&#34;required-r-packages&#34;&gt;Required R-packages&lt;/h4&gt;
&lt;p&gt;In principle, everything we&amp;rsquo;ll discuss can be accomplished using functions that come pre-loaded in base R, but we&amp;rsquo;ll make use of the &lt;em&gt;bbmle&lt;/em&gt; package for a few conveniences at the very end.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# install.packages(&amp;#39;bbmle&amp;#39;, dependencies = TRUE) # use to install if needed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;library&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bbmle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## Loading required package: stats4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;fundamentals&#34;&gt;Fundamentals&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re going to start by assuming our focal response variable doesn&amp;rsquo;t vary in response to any possible covariates at all.
Doing so is useful in helping us think about the different types of probability distributions there are, and which may be best in representing the type of stochastic processes that are likely to have generated our data.&lt;/p&gt;
&lt;h3 id=&#34;prob_dist&#34;&gt;Probability distributions&lt;/h3&gt;
&lt;p&gt;Our two example data sets above share some things in common because they both represent (or are derived from) counts of things.
Counts are integer-valued (i.e. 0, 1, 2, 3, &amp;hellip;) and can&amp;rsquo;t be negative.
The proportion of fertilized ovules is derived from two counts:
the count of fertilized ovules and the total count of available ovules.
Count data are a very common type of data in ecology, so we&amp;rsquo;ll focus on them for our purposes.&lt;/p&gt;
&lt;p&gt;Among the simplest and most appropriate probability distributions to represent such data are the &lt;em&gt;Poisson&lt;/em&gt; and &lt;em&gt;binomial&lt;/em&gt; probability distributions.
Because they represent counts, they are &lt;em&gt;discrete&lt;/em&gt; distributions.&lt;/p&gt;
&lt;h4 id=&#34;the-poisson-distribution&#34;&gt;The Poisson distribution&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;Poisson distribution&lt;/em&gt; would be appropriate for our dataset in which the count of visitations is our response variable.
It is written as
$$ Pr(k|\lambda) = \frac{\lambda^k e^{-\lambda}}{k!}$$
and expresses the probability that $k$ events will occur in some interval of time (i.e. that the count of events will be equal to $k$) given that the process responsible for generating the events occurs at a constant mean rate $\lambda$.
You can read $Pr(k|\lambda)$ as the probability of $k$ events &lt;em&gt;given&lt;/em&gt; parameter $\lambda$.
The symbol $e$ represents an exponential (i.e. Euler&amp;rsquo;s number: 2.718&amp;hellip;) and the symbol $!$ represents the factorial function (e.g., $4! = 4 \times 3 \times 2 \times 1$).&lt;/p&gt;
&lt;p&gt;Below, with data on replicate counts of $k$, we will presume that the Poisson applies and then estimate the value of &lt;em&gt;parameter&lt;/em&gt; $\lambda$ that is most likely to have generated those counts.
Intuitively, a higher underlying value of parameter $\lambda$ will result in higher $k$ counts when we draw from this distribution repeatedly
(i.e. when we obtain samples from the data-generating process).
In fact, $\lambda$ reflects the count that we &lt;em&gt;expect&lt;/em&gt; to observe on average across many such draws (a fact that we will prove later).&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/prob_Poiss-1.png&#34; width=&#34;576&#34; /&gt;
&lt;h4 id=&#34;the-binomial-distribution&#34;&gt;The Binomial distribution&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;binomial distribution&lt;/em&gt; would be appropriate for our dataset in which the proportion of fertilized ovules is our response variable.
More specifically, it will be appropriate after we re-express the number of counts (number of fertilized ovules) expected under the binomial distribution as a proportion of the maximum number of counts possible (the total number of ovules in a flower).
The binomial is written as
$$
Pr(k, n|p) = {n \choose k} p^k (1-p)^{n-k} = \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}
$$
and expresses the probability of observing $k$ events in a total of $n$ tries given that each event either does or does not happen with constant probabilities $p$ and $1-p$ respectively.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;
The first part of the equation, ${n \choose k}$, is read as &amp;ldquo;$n$ choose $k$&amp;rdquo;.
For example, given $n$ available ovules in a flower, $k$ of them are successfully pollinated.
(The proportion $k/n$ thus corresponds to our measure of fertilization success.) The larger the probability $p$ of success and the larger the number of $n$ tries, the larger that the average count of $k$ successes will be over replicate samples from the data-generating process.&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/prob_Binom-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;Note that in contrast to the Poisson distribution where the potential count $k$ could be arbitrarily high, $k$ is bounded by the maximum possible value of $n$ under the binomial distribution.
Although the average process rate is assumed constant under both distributions, a flower can be visited many times ($k \geq 0$), making the Poisson more appropriate.
On the other hand, an ovule that has already been fertilized can&amp;rsquo;t be fertilized again (thus $0 \leq k \leq n$), making the binomial more appropriate for our second experiment.&lt;/p&gt;
&lt;h4 id=&#34;other-probability-distributions&#34;&gt;Other probability distributions&lt;/h4&gt;
&lt;p&gt;There are &lt;a href=&#34;https://distribution-explorer.github.io/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dozens of probability distributions available&lt;/a&gt;, but the Poisson and binomial are arguably the simplest and most useful (and thus among the most commonly assumed in ecology).
One reason for this is that they have only a single free parameter ($\lambda$ and $p$, respectively) given that $n$ is usually known.
These parameters determines not only the expected (mean) value of the distribution of the $k$ counts but also their variance.
(For the Poisson, both the mean and the variance equal $\lambda$,
while for the binomial the mean is $np$ and the variance is $np(1-p)$.)&lt;/p&gt;
&lt;p&gt;Other distributions allow the mean and the variance to be separated.
For example, the &lt;em&gt;negative binomial&lt;/em&gt; and &lt;em&gt;beta-binomial&lt;/em&gt; are discrete examples that generalize the Poisson and binomial distributions to accommodate the common occurrence of over-dispersion (e.g., a variance that is larger than the mean).&lt;/p&gt;
&lt;p&gt;Another distribution that you&amp;rsquo;re probably much more familiar with is the &lt;em&gt;Normal&lt;/em&gt; (a.k.a. &lt;em&gt;Gaussian&lt;/em&gt;) distribution.
The Normal distribution has two parameters $\mu$ and $\sigma$ that respectively determine its mean and variance.
The Normal distribution has a long history of use in ecology and statistics, and is a biologically-appropriate distribution in many circumstances (thanks to the power of the Central Limit Theorem).
It could even be appropriate in the circumstances of our two example data sets because both the Poisson distribution and the binomial distribution converge on the Normal distribution (see the figures above).
But that is only true for sufficiently large $\lambda$, and sufficiently large $n$ and intermediate $p$, respectively.&lt;/p&gt;
&lt;p&gt;In our context of &amp;ldquo;mechanistic&amp;rdquo; models, we often have data from situations where the conditions that lead to a Normal distribution are far from satisfied.
For example, when flower abundances are very low, visitation rates ($\lambda$) will be very low. For small $\mu$ and large $\sigma$, the Normal distribution will also give negative values, which often don&amp;rsquo;t make sense for ecological data sets (you can&amp;rsquo;t have negative counts).
Finally, in the context of fitting mechanistic models, we often don&amp;rsquo;t care about estimating the variance; our goal is typically to estimate the parameter values that maximize a model&amp;rsquo;s fit to the mean of our data.
In that sense, having an extra variance parameter, such as $\sigma$ of the Normal distribution, is actually a nuisance (they&amp;rsquo;re often called &amp;ldquo;nuisance parameters&amp;rdquo;) because it means the model is more complex and thus potentially more challenging to fit.
Just as (or even more) importantly, these nuisance parameters can lead to biased estimates of the other &amp;ldquo;mechanistic&amp;rdquo; parameters that we actually do care about, especially when sample sizes are not large (as is often the case in Ecology).&lt;/p&gt;
&lt;h3 id=&#34;lik_fun&#34;&gt;Likelihood functions&lt;/h3&gt;
&lt;p&gt;Probability distributions express the probability of an outcome given their parameter(s).
For the Poisson distribution we therefore wrote $Pr(k, n | \lambda)$, but more generically for any discrete distribution we&amp;rsquo;ll write $Pr(y | \theta)$,
using $y$ to represent outcomes and $\theta$ the parameter(s).
$Pr(y | \theta)$ is referred to as a &lt;em&gt;probability mass function&lt;/em&gt;;
the input is the parameter values in $\theta$ and the output is the probability of any potential outcome $y$.&lt;/p&gt;
&lt;p&gt;In contrast, when we have data and want to estimate the parameters of a presumed model, we want the reverse.
We then talk of wanting to quantify the &lt;em&gt;likelihood&lt;/em&gt; of any potential parameter value given the data.
We therefore write $\L(\theta | y)$.
$\L(\theta | y)$ is referred to as the &lt;em&gt;likelihood function&lt;/em&gt;;
the input is an outcome and the output is the likelihood that a potential parameter value could have generated that outcome.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If we observe a given outcome then the &lt;em&gt;most likely&lt;/em&gt; parameter value to have generated that outcome is the one that &lt;em&gt;maximizes&lt;/em&gt; the probability of the outcome.&lt;/strong&gt;
We therefore define the likelihood function to be the probability mass function, setting
$$
\L(\theta | y) = Pr(y | \theta).
$$
This may seem silly, but it&amp;rsquo;s a conceptually important step.&lt;/p&gt;
&lt;p&gt;Assuming the Poisson specifically, we therefore have
$$
\L(\lambda | k)  = Pr(k | \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}.
$$&lt;/p&gt;
&lt;p&gt;The method of maximum likelihood is a matter of finding the value of the parameter that will maximize the output of the likelihood function when we input observed outcomes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#Prob_mass_dens&#34;&gt;Note on discrete vs. continuous probability distributions&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;max_lik&#34;&gt;Maximum likelihood&lt;/h2&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;As just stated,
maximum likelihood estimation is a matter of the finding the value of the parameter(s) that maximize the likelihood of having observed our data.
Let&amp;rsquo;s walk through how we would do that, first assuming we have only a single data point and then assuming we have many.&lt;/p&gt;
&lt;h4 id=&#34;single-observation&#34;&gt;Single observation&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s say that we sat in a meadow,
watched a single flower for some amount of time,
and observed a total of $k=10$ pollinator visits in that time.
Presuming the Poisson to be an appropriate representation of the visitation process,
we can use our data to determine the most likely value for $\lambda$.&lt;/p&gt;
&lt;p&gt;Remember that $\lambda$ reflects the underlying visitation rate (visits per time) of the process and then think about the following plots of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the probability mass $Pr(k | \lambda )$ as a function of potential $k$ values (with our data $k=10$ highlighted) for various values of $\lambda$,
and&lt;/li&gt;
&lt;li&gt;the likelihood $\L(\lambda | k)$ as a function of $\lambda$ for $k=10$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that the illustrated likelihood function on the right has a maximum at a count of exactly $k=10$.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;
That is, given our observation of $k=10$ visits, the most likely value of the rate parameter $\lambda$ is 10.
(Hopefully that&amp;rsquo;s not all too surprising;
remember that $\lambda$ reflects the expected (mean) value of a Poisson-distributed variable.)&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/lik_Poiss_single-1.png&#34; width=&#34;768&#34; /&gt;
&lt;h4 id=&#34;multiple-observations&#34;&gt;Multiple observations&lt;/h4&gt;
&lt;p&gt;What do we do when we have not just one but multiple observations?
That is, what if we had a field assistant who also watched a single flower
and counted $k=15$ visits in the same amount of time?
We therefore have two observations, $k = {10, 15}$, to consider.&lt;/p&gt;
&lt;p&gt;If the probability of the first observation is $Pr(k=10 | \lambda)$
and the probability of the second observation is $Pr(k=15 | \lambda)$,
then the probability of observing both is their product,
$Pr(k={10, 15} | \lambda) = Pr(10 | \lambda) \cdot Pr(15 | \lambda)$.&lt;/p&gt;
&lt;p&gt;The same is true for likelihoods.
To get the overall (&amp;ldquo;joint&amp;rdquo;) likelihood of $\lambda$ given &lt;em&gt;both&lt;/em&gt; observations,
we simply multiply their likelihoods.
Thus
$\L(\lambda | k={10, 15}) = \L(\lambda | k=10) \cdot  \L( \lambda | k=15)$.
More specifically,
$$
\L(\lambda | k={10, 15})  =
\frac{\lambda^{10} e^{-\lambda}}{10!}
\cdot
\frac{\lambda^{15} e^{-\lambda}}{15!}.
$$
As the next figure shows,
$\L{\lambda | k={10, 15})$ has a maximum at $\lambda = 12.5$
(which corresponds to the average of the two observations).&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/lik_Poiss_two-1.png&#34; width=&#34;384&#34; /&gt;
&lt;p&gt;For an arbitrary number of $n$ observations (i.e. $k={ k_1, k_2, \ldots, k_n}$),
we have a product of $n$ observation-specific likelihoods.
Our joint likelihood function with which to determine $\lambda$ thus becomes a function of these observation-specific likelihoods,
$$
\L(\lambda |k)
= \prod_{i=1}^n \frac{\lambda^{k_i} e^{-\lambda}}{k_i!}
$$
where the symbol $\prod_{i=1}^n$ denotes the product of elements $i=1$ to $n$.
That is,
$$
\prod_{i=1}^n \frac{\lambda^{k_i} e^{-\lambda}}{k_i!}
= \frac{\lambda^{k_1} e^{-\lambda}}{k_1!}
\cdot
\frac{\lambda^{k_2} e^{-\lambda}}{k_2!}
\cdot
\ldots
\cdot
\frac{\lambda^{k_n} e^{-\lambda}}{k_n!}.
$$&lt;/p&gt;
&lt;h3 id=&#34;why-the-negative-log-likelihood&#34;&gt;Why the negative log-likelihood?&lt;/h3&gt;
&lt;p&gt;We want the value of $\lambda$ that maximizes the joint likelihood over all our observations.
But we&amp;rsquo;ve created a numerical problem in the previous step.
Probabilities are numbers bounded by 0 and 1,
and therefore so are likelihoods.
As a result, by multiplying all those likelihoods together, our joint likelihood will become a smaller and smaller number with every data point we add to our dataset!
In fact, as our number of observations grows their joint likelihood becomes a vanishingly small number that neither our heads nor computers can deal with!&lt;/p&gt;
&lt;p&gt;The solution is the logarithm.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
Processes that are multiplicative on the natural scale become additive on the logarithmic scale, i.e.
$$
\ln ( x\cdot y \cdot z) = \ln(x) + \ln(y) + \ln(z).
$$
Therefore,
$$
\ln \L(\lambda |k)
= \ln \left (\prod_{i=1}^n \frac{\lambda^{k_i} e^{-\lambda}}{k_i!} \right)
= \sum_i^n \ln \left (\frac{\lambda^{k_i} e^{-\lambda}}{k_i!} \right)
$$
where
$$
\sum_i^n \ln \left (\frac{\lambda^{k_i} e^{-\lambda}}{k_i!} \right)
= \ln \left(\frac{\lambda^{k_1} e^{-\lambda}}{k_1!} \right )
+
\ln \left( \frac{\lambda^{k_2} e^{-\lambda}}{k_2!} \right)
+
\ldots
+
\ln \left (\frac{\lambda^{k_n} e^{-\lambda}}{k_n!} \right ).
$$
Thus the &lt;em&gt;log-likelihood&lt;/em&gt; increases (rather than decreases) in value as the number of observations increases.
Big numbers are no problem to deal with, so we&amp;rsquo;ve successfully avoided our numerical problem.&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/neglog-1.png&#34; width=&#34;384&#34; /&gt;
&lt;p&gt;But taking the logarithm has a side effect because the logarithm of a number between 0 and 1 returns a &lt;em&gt;negative&lt;/em&gt; number.
Since each of the individual likelihoods is between 0 and 1,
a side-effect of taking $\ln \L$ is that we&amp;rsquo;re now adding up lots of negative numbers.
That&amp;rsquo;s not a problem because we can just take the negative log-likelihood to get back to a positive number.
But importantly, as a consequence,
&lt;strong&gt;in order to &lt;em&gt;maximize&lt;/em&gt; the likelihood $\L$, we need to &lt;em&gt;minimize&lt;/em&gt; the negative log-likelihood $-\ln \L$&lt;/strong&gt;
to find the parameter value that best fits the data.&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/nlnlik_Poiss-1.png&#34; width=&#34;384&#34; /&gt;
&lt;h3 id=&#34;finding-the-mles&#34;&gt;Finding the MLEs&lt;/h3&gt;
&lt;p&gt;For some models it is possible to determine the values of the parameters that maximize the likelihood function analytically.
That is, it&amp;rsquo;s possible to solve for the maximum likelihood parameters
(i.e. the maximum likelihood estimators (&amp;ldquo;MLEs&amp;rdquo;)).
Doing so for $\lambda$ of the Poisson, we can
(i) build intuition to better understand what numerical methods (&amp;ldquo;optimizers&amp;rdquo;) are about, and
(ii) prove that the MLE for $\lambda$ of the Poisson is the mean of the $k$ observations (as I simply proclaimed above).
However, I&amp;rsquo;ll relegate the latter of these to the &lt;a href=&#34;of_potential_utility_or_interest&#34;&gt;Of Potential Utility or Interest&lt;/a&gt; page.&lt;/p&gt;
&lt;h4 id=&#34;max_lik_math&#34;&gt;Analytical intuition&lt;/h4&gt;
&lt;p&gt;We&amp;rsquo;ll do this rather abstractly&amp;hellip;&lt;/p&gt;
&lt;p&gt;Suppose we have some arbitrary function, like this polynomial describing a parabola
$f(x) = a + b (x-c)^2$ depicted in the following figure.
The way to find the value of $x$ where $f(x)$ is at its minimum value is to find the value of $x$ where the slope of $f(x)$ (with respect to $x$) is zero.&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/fig_poly-1.png&#34; width=&#34;384&#34; /&gt;
&lt;p&gt;How do we determine the slope of $f(x)$ as a function of $x$?
We take the derivative of $f(x)$ with respect to $x$: $\frac{d ; f(x)}{dx}$.
For our polynomial, $\frac{d ; f(x)}{dx} = 2b(x-c)$,
which even R has the ability to solve symbolically:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;expression&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;^2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## b * (2 * (x - c))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Setting $\frac{d; f(x)}{dx} = 2b(x-c) = 0$
(since that&amp;rsquo;s where $f(x)$ will have its minima (or maxima)&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;),
we use a little algebra to solve for $x$:
$$
2b(x-c)=0 \implies 2bx-2bc=0 \implies 2bx = 2bc \implies x=c
$$
(which correctly equals the location of the minimum at $x=2$,
as shown in the above figure).
We can do the same thing for likelihood functions to obtain their MLEs.
Doing so often leads to useful insight into the processes and variables that cause parameter estimates to change in value.&lt;/p&gt;
&lt;h4 id=&#34;max_lik_optim&#34;&gt;Numerical optimization&lt;/h4&gt;
&lt;p&gt;There are two basic parts to finding the MLE by numerical means:
coding the negative log-likelihood function and choosing the optimization method to find its minimum.&lt;/p&gt;
&lt;h5 id=&#34;coding-the-likelihood&#34;&gt;Coding the likelihood&lt;/h5&gt;
&lt;p&gt;For the Poisson, we have the negative log-likelihood function&lt;/p&gt;
&lt;p&gt;$$
-\ln \L(\lambda | k) =
\sum_i^n \ln \left (\frac{\lambda^{k_i} e^{-\lambda}}{k_i!} \right).
$$
You could certainly write code to define an R function to implement this calculation
(see the &lt;a href=&#34;of_potential_utility_or_interest&#34;&gt;Of Potential Utility or Interest&lt;/a&gt; page),
and sometimes it&amp;rsquo;s necessary to do that for more complex likelihoods.
But R has built-in functions for most probability distributions that are robust, fast, and easy to use.
These R functions are the &amp;ldquo;density functions&amp;rdquo; of the probability distributions.&lt;/p&gt;
&lt;p&gt;For example, while R&amp;rsquo;s function &lt;code&gt;runif(n, min, max)&lt;/code&gt; draws &lt;code&gt;n&lt;/code&gt; random values from the uniform probability distribution bounded by &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt; parameter values,
the corresponding density function &lt;code&gt;dunif(x, min, max)&lt;/code&gt; returns the likelihood of the value &lt;code&gt;x&lt;/code&gt; given &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt;.
For the Poisson, we have &lt;code&gt;dpois(x, lambda)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that, by default, the likelihood is returned on the natural scale.
To obtain the &lt;em&gt;negative log&lt;/em&gt;-likelihood of &lt;code&gt;x&lt;/code&gt;, we use &lt;code&gt;-dpois(x, lambda, log = TRUE)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;x&lt;/code&gt; represents a vector of values (multiple observations), we obtain a vector of corresponding negative log-likelihoods (one for each observation).
Since we want the overall joint negative log-likelihood of all the observations,
we have to sum their negative log-likelihoods together: &lt;code&gt;-sum(dpois(x, lambda, log = TRUE))&lt;/code&gt;.
We can define our own R function to implement this and use it more conveniently.
Notice that we define the function with only &lt;code&gt;lambda&lt;/code&gt; as an argument.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Define the negative log-likelihood&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;nlL.pois&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lambda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;dpois&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lambda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Continuing with our dataset of $k = {10, 15}$ observations from above,
we can apply our function as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Define k globally for our nlL.pois() function to use.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Demonstrate use of nlL.pois() for two hypothetical values of lambda.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;nlL.pois&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## [1] 5.056302
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;nlL.pois&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;12.4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## [1] 4.861272
&lt;/code&gt;&lt;/pre&gt;&lt;h5 id=&#34;optimization&#34;&gt;Optimization&lt;/h5&gt;
&lt;p&gt;Given data $k={10, 15}$, we now need to find the value of $\lambda$ that minimizes our negative log-likelihood function &lt;code&gt;nlL.pois()&lt;/code&gt;.
For many circumstances, the R function &lt;code&gt;optim()&lt;/code&gt; is the go-to function for doing so because it is robust and has several standard optimization methods and control options to choose from.
It comes in base R, so there&amp;rsquo;s no package to load.&lt;/p&gt;
&lt;p&gt;At minimum, we have to pass &lt;code&gt;optim(par, fn, ...)&lt;/code&gt; where &lt;code&gt;fn&lt;/code&gt; is the function we wish to minimize and &lt;code&gt;par&lt;/code&gt; is a guess at an initial parameter value at which the optimizer will start its search.
(You can ignore the warning message in the following example.&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;init.par&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;init.par&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nlL.pois&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## Warning in optim(init.par, nlL.pois): one-dimensional optimization by Nelder-Mead is unreliable:
## use &amp;#34;Brent&amp;#34; or optimize() directly
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## $par
## lambda 
##   12.5 
## 
## $value
## [1] 4.860468
## 
## $counts
## function gradient 
##       34       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The output of &lt;code&gt;optim()&lt;/code&gt; includes the MLE for our parameter $\lambda$,
the &lt;code&gt;value&lt;/code&gt; of our negative log-likelihood function at the MLE,
and a &lt;code&gt;convergence&lt;/code&gt; error code (with 0 indicating success).&lt;/p&gt;
&lt;h3 id=&#34;max_lik_class&#34;&gt;Let&amp;rsquo;s try it&lt;/h3&gt;
&lt;h5 id=&#34;repeat-for-yourself&#34;&gt;Repeat for yourself&lt;/h5&gt;
&lt;p&gt;In-class 
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 10-15 minutes&lt;/p&gt;
&lt;p&gt;Download &lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_1.R&#34;&gt;ModelFitting_1.R&lt;/a&gt; to implement what we just did with the $k={10, 15}$ data set and Poisson distribution.
Then try generating different data sets that vary in their sample size to see how close you get to the true value of $\lambda$ with which you generated the data.
(We&amp;rsquo;ll address estimation uncertainty through the use of confidence intervals later.)&lt;/p&gt;
&lt;h5 id=&#34;real-binomial-data&#34;&gt;Real binomial data&lt;/h5&gt;
&lt;p&gt;Your actual challenge, however, is to generate your own data and write your own code to analyse it.&lt;/p&gt;
&lt;p&gt;The data set will be contributed to by everyone in the class.
Each of you will take one (or more) &lt;em&gt;Delphinium&lt;/em&gt; seed pods and count both the total number of ovules and the number that were successfully fertilized.
Enter your data on &lt;a href=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vTyyqthYKtedUdgRCOE37ec-oA4TzY6Mq8glR9bWr8ORhGQjWZlkeIuM5AgdGa8-zHE9pJma8C3n4_n/pub?gid=0&amp;amp;single=true&amp;amp;output=csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this GoogleSheet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As soon as you&amp;rsquo;re done with data entry, download the data as a &lt;em&gt;csv&lt;/em&gt; file and start analyzing it.
(You can re-download it again after everyone&amp;rsquo;s finished entering their data.
You only need one data point to start fitting this one-parameter model!)&lt;/p&gt;
&lt;p&gt;Rather than using a Poisson, you&amp;rsquo;ll want to assume a binomial process.
Your goal is to estimate $p$, the average probability of fertilization success.
Download &lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_2.R&#34;&gt;ModelFitting_2.R&lt;/a&gt; to help you get going.&lt;/p&gt;
&lt;h2 id=&#34;model-fitting&#34;&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;Up to this point, we have assumed that the process that is reflected in our data,
be it the flower visitation rate $\lambda$ or the fertilization success probability $p$,
is well-represented as being a constant value that is independent of any other contributing factor.
Visually, if we actually had data like that introduced at the very start of today, our models so far would correspond to the horizontal lines in the following figures.
That is, we&amp;rsquo;d be assuming that visitation counts are independent of plant density,
and that the proportions of ovules fertilized are independent of visitation count.
All variation is noise.&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/motivation_constant-1.png&#34; width=&#34;768&#34; /&gt;
&lt;p&gt;We can think of these models of constant $\lambda$ and $p$ as being our simplest 1-parameter &amp;ldquo;null&amp;rdquo; models;
if you didn&amp;rsquo;t have any other information, your best prediction for the expected value of a future data point would be the mean of current data.
Clearly, these data visualizations indicate that these null models do not capture all the information that a more complex model could potentially capture.
Although there is plenty of noise, the visitation rate and the fertilization success probability appear to be increasing functions of plant density and visitation count, respectively.
That is, $\lambda$ would probably be better considered to be a function of plant density and $p$ would probably be better considered to be a function of visitation count.
So how do we go about encoding that in our models?&lt;/p&gt;
&lt;p&gt;Well, there are many ways that we could do it.
We could go the route of using &lt;em&gt;GLMs&lt;/em&gt; and &lt;em&gt;GAMs&lt;/em&gt; that permit us to continue to appropriately represent non-Gaussian error structures and potentially non-linear relationships
(see the &lt;a href=&#34;of_potential_utility_or_interest&#34;&gt;Of Potential Utility or Interest&lt;/a&gt;).
But, as motivated above, our goal is to fit mechanistic models instead.&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&#34;mechanistic-models&#34;&gt;Mechanistic models&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s consider our visitation count vs. plant density dataset.
Even if the total number of pollinators that are flying in the sky is invariant (constant) with respect to the density of our focal plant species, it seems somewhat intuitive that we should see more total visits to the focal plant in areas where the focal plant&amp;rsquo;s density is higher.
That is, setting aside targeted searching and all other kinds of potential pollinator behavior, it should be true that the more plants there are, the greater the chance that a pollinator will bump into any one of them by random chance alone.
The mathematically simplest model, we&amp;rsquo;ll call it $f_1$, to describe such a presumed dependence of $\lambda$ on $P$ would be
$$
\text{Visitation rate }\lambda = f_1(P) = a P,
$$
where a new parameter $a$ encapsulating what may potentially be several aspects of the pollinator&amp;rsquo;s search effort, including its flying speed, its preference for the focal plant, etc..
The higher the value of $a$, the steeper the slope of the assumed biological relationship between $\lambda$ and $P$ and hence the steeper the slope of observed visitation counts versus plant density we expect to see in our data.
In the literature on Consumer-Resource theory, this model is referred to as the &lt;em&gt;Holling Type I functional response&lt;/em&gt;.&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/motivation_prop-1.png&#34; width=&#34;336&#34; /&gt;
&lt;p&gt;A more flexible (and possibly more realistic) second model describes $\lambda$ as increasing with $P$ in a monotonically-saturating fashion to encode the hypothesis that visitation rates can&amp;rsquo;t increase with $P$ forever.
That is, while visitation rates are likely determined by search effort and plant density at low $P$ (just like for model $f_1$), they are probably bounded at some maximum possible visitation rate at high values of $P$.
If, for example, a pollinator spends $h$ minutes per visit to each encountered flower, then the maximum possible visitation rate at high values of $P$ (where there is effectively no time needed to search and find the next flower) is $1/h$ visits per minute.
The two-parameter model
$$
\text{Visitation rate }\lambda = f_2(P) = \frac{a P}{1 + a h P}
$$
encapsulates this.
The more plants there are, the more that each pollinator spends time &amp;ldquo;handling&amp;rdquo; rather than searching and encountering more plants.
In the literature, this model is referred to as the &lt;em&gt;Holling Type II functional response&lt;/em&gt;.
(An alternative but deterministically-equivalent parameterization is referred to as the &lt;em&gt;Michaelis-Menten&lt;/em&gt; model.)
Note that when $h=0$, model $f_2$ collapses back to model $f_1$.
Model $f_1$ is therefore nested within $f_2$.&lt;/p&gt;
&lt;p&gt;Lastly, an even more flexible (and possibly even more realistic) third model describes $\lambda$ as increasing with $P$ in a sigmoidal fashion to encode the hypothesis that a pollinator&amp;rsquo;s search rate may additionally be limited at low $P$.
For example, the rate at which a pollinator searches for plants may itself increase with plant density; they may develop an ever better &amp;ldquo;search image&amp;rdquo; as they encounter more plants.
The three-parameter model
$$
\text{Visitation rate }\lambda = f_3(P) = \frac{a P^\theta}{1 + a h P^{\theta}}
$$
encapsulates this when the new parameter $\theta$ (theta) takes on values greater than 1.
This model is most appropriately referred to as the &lt;em&gt;Holling-Real Type III functional response&lt;/em&gt;.&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;
Note that when $\theta=1$, model $f_3$ collapses back to model $f_2$ since $x^1 = x$.&lt;/p&gt;
&lt;h3 id=&#34;modifying-the-likelihood&#34;&gt;Modifying the likelihood&lt;/h3&gt;
&lt;p&gt;To fit the three $f_1-f_3$ models to our data we need to incorporate them into our likelihood function.
To do so we insert each of them in for $\lambda$.
Many refer to the resulting statistical model (i.e. the likelihood function) as being composed of a &lt;em&gt;deterministic skeleton&lt;/em&gt; and a &lt;em&gt;stochastic shell&lt;/em&gt;.
For example, for model $f_1$ we have
$$
-\ln \L_1(a | k, P)
= \sum_i^n \ln \left (\frac{f_1^{k_i} e^{-f_1}}{k_i!} \right)
= \sum_i^n \ln \left (\frac{(a P)^{k_i} e^{-a P}}{k_i!} \right).
$$
Notably, unlike our original negative log-likelihood function to which we supplied only the response variable data (visitation counts $k = {k_1, k_2, \ldots, k_n }$) in order to estimate the constant $\lambda$, we must now supply the both response variable data and the covariate data (of corresponding plant densities $P = {P_1, P_2, \ldots, P_n }$) in order to estimate up to three constants ($a$, $h$, and $\theta$) for the three models.&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;
But because $P$ is the same for all models, we can just leave it defined in R&amp;rsquo;s global environment (rather than defining it within the function&amp;rsquo;s internal environment).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll need a little more dummy data, so let&amp;rsquo;s create it using &lt;code&gt;runif()&lt;/code&gt; and &lt;code&gt;rpois()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;set.seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.33&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# True value of &amp;#39;a&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Sample size&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;runif&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Plant densities&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;NULL&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Visitation counts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;k[i]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rpois&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;P[i]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Generate a k for each P assuming Type 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;data.frame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;round&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##       P  k
## 1  26.6  6
## 2  37.2  9
## 3  57.3 22
## 4  90.8 33
## 5  20.2  6
## 6  89.8 33
## 7  94.5 26
## 8  66.1 18
## 9  62.9 19
## 10  6.2  1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now we define the negative log-likelihood for the $f_1$ and use &lt;code&gt;optim()&lt;/code&gt; to estimate the MLE for its $a$ parameter.
(Again we&amp;rsquo;ll ignore the warning for this 1-dimensional model.)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;nlL.pois.f1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;par&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;par[&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;dpois&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Estimate &amp;#39;a&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;par&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;             &lt;span class=&#34;n&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nlL.pois.f1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## Warning in optim(par = list(a = 0.2), fn = nlL.pois.f1): one-dimensional optimization by Nelder-Mead is unreliable:
## use &amp;#34;Brent&amp;#34; or optimize() directly
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## $par
##         a 
## 0.3136914 
## 
## $value
## [1] 24.37161
## 
## $counts
## function gradient 
##       28       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# True &amp;#39;a&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## [1] 0.33
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Estimated &amp;#39;a&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;par&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##         a 
## 0.3136914
&lt;/code&gt;&lt;/pre&gt;&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/motivation_type1-1.png&#34; width=&#34;336&#34; /&gt;
&lt;h3 id=&#34;lets-try-it&#34;&gt;Let&amp;rsquo;s try it&lt;/h3&gt;
&lt;h5 id=&#34;repeat-for-yourself-1&#34;&gt;Repeat for yourself&lt;/h5&gt;
&lt;p&gt;In-class 
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 30-45 minutes&lt;/p&gt;
&lt;p&gt;Download &lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_3.R&#34;&gt;ModelFitting_3.R&lt;/a&gt;.
The first part generates a random dataset.
It then repeats the fitting of the $f_1$ model that we just did.&lt;/p&gt;
&lt;p&gt;Your challenge is expand on the script to fit the $f_2$ and $f_3$ models to the same dataset.
Summarize what you learn by comparing the three models&#39;
(1) parameter MLEs to each other and the values with which the dataset was generated, and
(2) their minimized negative log-likelihoods (i.e. the value of their negative log-likelihood at their parameter MLEs).
Which model&amp;rsquo;s parameters got closest to the truth?
Which model is the &lt;em&gt;best-fitting&lt;/em&gt;?
(Is it a higher or a lower negative log-likelihood that is better?)
Which model is the &lt;em&gt;best-performing&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;But before you get going, take note of two things about&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Coding the likelihood, and&lt;/li&gt;
&lt;li&gt;Initial values &amp;amp; NaNs.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;coding-the-likelihood-1&#34;&gt;Coding the likelihood&lt;/h4&gt;
&lt;p&gt;The function we used to define the negative log-likelihood for the $f_1$ model above had only a single &lt;code&gt;par&lt;/code&gt; argument from which we &amp;ldquo;extract&amp;rdquo; parameter &lt;code&gt;a&lt;/code&gt; using &lt;code&gt;a &amp;lt;- par[&#39;a&#39;]&lt;/code&gt;.
(That is, we&amp;rsquo;ve added a line of code relative to what we did when we were just focused on $\lambda$.)
You&amp;rsquo;ll want to do the same thing when defining the likelihood functions for models $f_2$ and $f_3$; your likelihood functions must have only one &lt;code&gt;par&lt;/code&gt; argument from which each parameter is extracted.&lt;/p&gt;
&lt;h4 id=&#34;initial-values--nans&#34;&gt;Initial values &amp;amp; NaNs&lt;/h4&gt;
&lt;p&gt;In general, the more data you have and the simpler the model, the less sensitive the optimization will be to your initial parameter value guess.
In our simple example case involving $f_1$ and a linear relationship, almost any reasonable initial value for $a$ would have worked.&lt;/p&gt;
&lt;p&gt;In slightly more complex cases, you can often get a good enough guess by eyeballing your data and thinking about how the parameters of your deterministic function should influence its shape.
For example, for the $f_2$ model, parameter $a$ controls the slope at the origin at $P=0$ while parameter $h$ controls the level of the asymptotic maximum value of $1/h$ as $P\rightarrow \infty$, both of which can be read of a plot of the data.
Otherwise, the best way to proceed is to fit a simpler model and to use its MLEs to inform your guess for the next more complex model.
(That is, fit $f_1$ first and use its estimate for $a$ to inform the initial value for $a$ in model $f_2$, etc.)&lt;/p&gt;
&lt;p&gt;In complex cases, it&amp;rsquo;s often wise to try out several different starting values to ensure that you always arrive at the same MLE values.
Otherwise you run the risk of your optimizer finding a &lt;em&gt;local&lt;/em&gt; (rather than &lt;em&gt;global&lt;/em&gt;) minimum in the likelihood surface (see below).&lt;/p&gt;
&lt;p&gt;You may get a negative log-likelihood output of &lt;code&gt;NaN&lt;/code&gt; if your initial guess (or a value attempted during the optimization) is incompatible with the data.
&lt;code&gt;optim()&lt;/code&gt; will tell you when that happens.
If that happens for the initial value, try again with a different value.
If it happens during the optimization, it&amp;rsquo;s typically not a problem if a convergence code of 0 is obtained.&lt;/p&gt;
&lt;h2 id=&#34;mod_comp&#34;&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;In general, the more parameters a model has (i.e. the more &lt;em&gt;parametrically complex&lt;/em&gt; it is), the better it will fit the data.
Indeed, a little simplistically speaking, if you have a model that has as many free parameters as you have data points, then your model can fit the data perfectly with no unexplained residual variation leftover!&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;
A model&amp;rsquo;s &lt;em&gt;fit&lt;/em&gt; to the data as reflected by its likelihood (or its $R^2$ value) is thus rarely useful on its own when we want to compete different models against each other to ask which is &amp;ldquo;the best&amp;rdquo;.
Rather, the principle of parsimony (Occam&amp;rsquo;s razor) motivates us to identify the best-&lt;em&gt;performing&lt;/em&gt; model.
Typically the best-performing model is the one that &lt;em&gt;maximizes fit while minimizing complexity&lt;/em&gt; relative to the other considered models.
If two models fit the data equally well but one uses fewer parameters to achieve that fit, then it is the better-performing model.
Currently, the most widely used tools for quantifying relative model performance are based on &lt;em&gt;information theory&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;inf_crit&#34;&gt;AIC &amp;amp; BIC&lt;/h3&gt;
&lt;p&gt;Two information-theoretic indices of model performance are by far the most popularly used in ecology:
the Akaike Information Criterion (&lt;em&gt;AIC&lt;/em&gt;)
and the Bayesian Information Criterion (&lt;em&gt;BIC&lt;/em&gt;, a.k.a. the Schwartz Information Criterion).
The practical difference between these three criteria is in how they quantify model complexity.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;AIC&lt;/em&gt; score of a model is calculated as twice the minimized negative log-likelihood plus twice the number of parameters $p$:
$$
AIC = - 2 \ln \hat{\L} + 2 p.
$$
Note two things:&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We&amp;rsquo;re using the modified symbol $\hat{\L}$ rather than just $\L$ to indicate that we are using the &lt;em&gt;minimized&lt;/em&gt; value of negative log-likelihood (i.e. its value at the model&amp;rsquo;s parameter MLEs).&lt;/li&gt;
&lt;li&gt;Because a lower $-\ln \hat{\L}$ value reflects a better fit to the data, adding a model&amp;rsquo;s parameter count with $+2p$ reflects a &lt;em&gt;complexity penalty&lt;/em&gt; that makes a model&amp;rsquo;s &lt;em&gt;AIC&lt;/em&gt; value larger.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly, the $BIC$ score of a model is calculated as&lt;/p&gt;
&lt;!-- $$ --&gt;
&lt;!-- AICc =  - 2 \ln \hat{\L} + 2 p + \frac{2 p (p + 1)}{n-p-1} --&gt;
&lt;!-- $$ --&gt;
&lt;!-- and --&gt;
&lt;p&gt;$$
BIC = - 2 \ln \hat{\L} +  \ln(n) p,
$$
where the logarithm of the dataset&amp;rsquo;s sample size $n$ also contributes to the complexity penalty term.
Therefore, larger datasets impose a greater penalty on more parametrically-complex models for &lt;em&gt;BIC&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For both criteria, the model with the &lt;em&gt;lowest&lt;/em&gt; score is considered the best-performing model.
Note that when comparing models the dataset cannot change.
All models have to be fit to the same dataset.
More specifically, although the covariates can be different for different models, the response variable data (e.g., our visitation counts) must remain the same.&lt;/p&gt;
&lt;h4 id=&#34;relative-performance&#34;&gt;Relative performance&lt;/h4&gt;
&lt;p&gt;Having identified the best-performing model, it is natural to ask just how much better the best-performing model is relative to the others.
We can do that by calculating their $\Delta$ differences.
That is, the performance of the $i^{th}$ model relative to the best performing model is calculated as
$$
\Delta_i = AIC_i - AIC_{min}.
$$
The best-performing model will thus have $\Delta_i = 0$ and all others will have $\Delta_i &amp;gt; 0$.
The bigger the difference, the better the best-performing model.
Because we like cut-offs&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;,
by rule-of-thumb convention,
a difference of at least 2 units is deemed to reflect &amp;ldquo;substantial&amp;rdquo; support in favor of the best-performing model,
a 2-4 unit difference is deemed as providing &amp;ldquo;strong&amp;rdquo; support&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;,
a 4-7 unit difference as &amp;ldquo;considerably less&amp;rdquo; support,
while models having $\Delta_i &amp;gt; 10$ are deemed to have essentially no support.&lt;/p&gt;
&lt;h3 id=&#34;lets-try-it-1&#34;&gt;Let&amp;rsquo;s try it&lt;/h3&gt;
&lt;p&gt;In-class 
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 10-15 minutes&lt;/p&gt;
&lt;p&gt;Adding to your updated copy of &lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_3.R&#34;&gt;ModelFitting_3.R&lt;/a&gt;, write functions for $AIC$ and $BIC$ and use them to identify the best-performing model among our three $f_1$ - $f_3$ models.
Do your inferences differ from your earlier evaluation of the models?&lt;/p&gt;
&lt;h3 id=&#34;alternatives&#34;&gt;Alternatives&lt;/h3&gt;
&lt;p&gt;Information criteria like &lt;em&gt;AIC&lt;/em&gt; and &lt;em&gt;BIC&lt;/em&gt; are popular because they are super easy to implement.
They also have the benefit of enabling the comparison of any set of models for which a likelihood can be calculated and the parameters can be counted (or estimated).
In particular, the models in the set need not be nested subsets of each other as is necessary for the alternative model-comparison approach of &lt;em&gt;Likelihood Ratio Tests&lt;/em&gt;.  (Our $f_1$ - $f_3$ models &lt;em&gt;are&lt;/em&gt; nested subsets, because $f_3$ can be reduced to $f_2$ which can be reduced to $f_1$, so likelihood ratio tests could be used.)&lt;/p&gt;
&lt;p&gt;There are also several other information criteria available.
The commonly-used &lt;em&gt;AICc&lt;/em&gt; attempts to account for a bias towards more complex models that occurs for &lt;em&gt;AIC&lt;/em&gt; when sample sizes are small.
For models that are linear in their parameters and have residuals that are well-approximated by a Normal distribution,
$$
AIC_c = AIC + \frac{2k(k+1)}{n-k-1}.
$$
Unfortunately there is no similarly simple equation for nonlinear models with non-Normal residuals.&lt;/p&gt;
&lt;p&gt;Other approaches for balancing fit and complexity include the
&lt;em&gt;Fisher Information Approximation&lt;/em&gt; (FIA) which considers not just a model&amp;rsquo;s parametric complexity but also its geometric complexity (i.e. how flexible it is),
and various forms of &lt;em&gt;cross-validation&lt;/em&gt; whereby subsets of a given data set are repeatedly left out of the fitting (&amp;ldquo;training&amp;rdquo;) process to be used as &amp;ldquo;test data&amp;rdquo; with which to evaluate the model&amp;rsquo;s predictive ability.
In the latter, unnecessarily complex models will over-fit the training data and will therefore perform poorly on the test data.&lt;/p&gt;
&lt;h3 id=&#34;philosophies&#34;&gt;Philosophies&lt;/h3&gt;
&lt;p&gt;Much has been written about the pros and cons, foundations and assumptions, epistomological and pragmatic differences between the various approaches to model comparison.
That&amp;rsquo;s particularly true for AIC vs. BIC.
The comparison is often framed as being rather absolute and clear (especially in the ecological literature), but my sense is that it is far more nuanced and muddy.&lt;/p&gt;
&lt;p&gt;Much of the discussion pertains to questions of motivation in comparing alternative models.
Is it to describe your data?  Is it to predict new data?  Is it to explain your data?
If it&amp;rsquo;s to predict, then what kind of prediction are you wanting to make (e.g., within-sample, out-of-sample, out-of-range, forecast, etc.)?
Conceptually, all questions will arrive at the same conclusion regarding which model is &amp;ldquo;best&amp;rdquo; when your data set is very large and sufficiently representative of all possibilities, but that is rarely if ever the case.
Therefore, choices about which approach to take have to be made.
For example, the derivation of AIC is motivated by a desire to maximize out-of-sample predictive ability
(i.e. minimizing the prediction error when predicting what equivalent data sets would look like).
Asymptotically (as sample sizes increase), AIC is equivalent to the leave-one-out form of cross-validation.
In contrast, the derivation of BIC is motivated by a desire to identify the &amp;ldquo;true&amp;rdquo; model
(i.e. to interpret or draw inferences about biology), and in that sense are more similar to leave-many-out forms of cross-validation and asymptotically converge on criteria like FIA.&lt;/p&gt;
&lt;p&gt;That said, most of the mathematical and statistical basis for contrasting the different approaches is limited to linear regression type models.
Thus, while I consider it important to go beyond what most ecologists seem to be doing
(i.e. ignore the differences and just go with AIC because everyone else is using it)
by thinking about your motivations and the trade-offs among them,
I would say that a fair degree of pragmatism remains warranted.&lt;/p&gt;
&lt;h2 id=&#34;uncertainty&#34;&gt;Uncertainty&lt;/h2&gt;
&lt;h3 id=&#34;likelihood-surfaces&#34;&gt;Likelihood surfaces&lt;/h3&gt;
&lt;p&gt;When we have more than one parameter, we talk about the likelihood function as describing a &amp;ldquo;likelihood surface&amp;rdquo;.
For a model with 2 parameters, we can easily visualize that surface in a single plot.
When a model has $p$ parameters there are $p$ dimensions to the likelihood surface.
Minimizing the negative log-likelihood to obtain the parameter MLEs is equivalent to finding the lowest point of that surface.
Doing so is not a problem when the surface is like a smooth, unimodal, round bowl shape with only one well-defined minimum point,
but such a shape is far from guaranteed for all models and datasets.
In fact, depending on the data and/or the model, a likelihood surface could exhibit many shapes that are different from that of a (multidimensional) bowl.&lt;/p&gt;
&lt;p&gt;The shape of the likelihood surface relates to issues of parameter- (and model-) identifiability,
and to attempts to quantify parameter uncertainty.&lt;/p&gt;
&lt;h3 id=&#34;par_ident&#34;&gt;Identifiability&lt;/h3&gt;
&lt;p&gt;The likelihood surface could be wavy and have multiple minima.
In that case our optimizer could potentially get stuck in a &lt;em&gt;local&lt;/em&gt; minimum,
unable to find the true MLE associated with the &lt;em&gt;global&lt;/em&gt; (lowest) minimum.
Different optimizers attempt to deal with this potential problem in different ways, but as alluded to above, it is often wise to start at different initial parameter values to confirm convergence to the same optimum.&lt;/p&gt;
&lt;p&gt;The surface could be relatively flat in one or more parameters.
The reasons for this include that there may be insufficient information in the data to constrain the values of the parameter(s) (i.e. all values are similarly likely);
a parameter&amp;rsquo;s MLE would thus come with large uncertainty.&lt;/p&gt;
&lt;p&gt;The surface could be relatively or even entirely flat for some combination of parameters.
This could happen because the model (or the data) are structured in such a way that two or more parameters are not (sufficiently) identifiable.
That is, parameters (or covariates) may co-vary in such a way that their independent effects cannot be distinguished.
In two-dimensions, this would look like a likelihood surface that has a valley running through it.
Each individual parameters apparent MLE could thus appear to be well-constrained (i.e. appear to have low uncertainty), but there could nonetheless be considerable uncertainty in their values.&lt;/p&gt;
&lt;p&gt;My sense is that likelihood surfaces with multiple minima most commonly occur for datasets that are too small for the task.
For small data sets, noise predominates and different parameter combinations fit different data points similarly well.&lt;/p&gt;
&lt;p&gt;Flat surfaces or surfaces with valleys, on the other hand, will not be resolved simply by having more data (if it is similar to the data already available).
Rather, what is typically needed is either more information and/or an alternatively formulated model structure.
More information can come from better-designed &amp;rsquo;experiments&amp;rsquo; that, for example,
(i) increase the range of values that the predictor variables exhibit (e.g., beyond what is &amp;ldquo;natural&amp;rdquo;), and
(ii) increase the independence (decreases the co-variation) among predictor variables.
Examples of alternative model structures include
(i) replacing two parameters that always co-occur as a product by a single compound parameter, and
(ii) rewritting a model in a mathematically equivalent (but statistically non-equivalent) formulation
(e.g., the Holling Type II versus the Michaelis-Menten models).
That said, these are relatively extreme, simple examples that represent a range of potential issues and solutions relating to identifability that you may encounter and where expert experience is often required.&lt;/p&gt;
&lt;h3 id=&#34;quantifying-uncertainty&#34;&gt;Quantifying uncertainty&lt;/h3&gt;
&lt;p&gt;When our goal is to make comparisons of model performance using information criteria,
then all we need from model fitting are the values of each model&amp;rsquo;s minimum negative log-likelihood associated with their parameter MLEs.
However, many times we&amp;rsquo;re not only interested in just doing model comparison or just obtaining the MLE values of the parameters.
Rather, we often would also like to characterize the statistical uncertainty (&amp;ldquo;clarity&amp;rdquo;) of the parameter estimates.
The most common way to do that is by determining the 95% confidence intervals of the MLEs.&lt;/p&gt;
&lt;p&gt;For that, we&amp;rsquo;ll make use of the &lt;code&gt;mle2()&lt;/code&gt; and &lt;code&gt;confint()&lt;/code&gt; functions from the &lt;code&gt;bbmle&lt;/code&gt; package. &lt;code&gt;mle2()&lt;/code&gt; uses &lt;code&gt;optim()&lt;/code&gt; as the default optimizer to determine the parameter MLEs, and &lt;code&gt;confint()&lt;/code&gt; takes the result and returns the parameter confidence intervals.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll continue on with the small $n=10$ dataset we generated when we demonstrated the fitting of the $f_1$ model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;data.frame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;round&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##       P  k
## 1  26.6  6
## 2  37.2  9
## 3  57.3 22
## 4  90.8 33
## 5  20.2  6
## 6  89.8 33
## 7  94.5 26
## 8  66.1 18
## 9  62.9 19
## 10  6.2  1
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;library&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bbmle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;nlL.pois.f1.bbmle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;dpois&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mle2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nlL.pois.f1.bbmle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## 
## Call:
## mle2(minuslogl = nlL.pois.f1.bbmle, start = list(a = 0.2))
## 
## Coefficients:
##         a 
## 0.3136831 
## 
## Log-likelihood: -24.37
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;confint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##     2.5 %    97.5 % 
## 0.2692366 0.3627788
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notice that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mle2()&lt;/code&gt; wants the negative log-likelihood function to have (all) the parameters as arguments (unlike &lt;code&gt;optim()&lt;/code&gt; which wanted a single &lt;code&gt;par&lt;/code&gt; argument containing the parameters);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mle2()&lt;/code&gt; has the order of the arguments for the negative log-likelihood and the list of initial parameter values reversed relative to &lt;code&gt;optim()&lt;/code&gt;; and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mle2()&lt;/code&gt; returns the (positive) log-likelihood rather than the negative log-likelihood. (I don&amp;rsquo;t know why.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;bbmle&lt;/code&gt; package also has several other potentially useful functions that we won&amp;rsquo;t get into.
Note that the above code is included at the bottom of &lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_3.R&#34;&gt;ModelFitting_3.R&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;end_result&#34;&gt;End result&lt;/h2&gt;
&lt;p&gt;We used our visitation count data (introduced at the very start) to fit three models relating visitation rates to plant density.
These models had either one, two, or three biologically-reasoned parameters that enabled the models to describe different types of relationships between observed visitation counts and plant density:  linear, saturating, and sigmoid.
The three models therefore reflected different expectations/hypotheses about the deterministic biological processes that could have generated the data,
but in fitting them we also attempted to stay true to the likely stochastic processes that introduced noise to the data.&lt;/p&gt;
&lt;p&gt;The results of our efforts are as follows:&lt;/p&gt;
&lt;img src=&#34;https://models4data2theory.github.io/courses/wkshp_interactions_files/figure-html/endresult_fit-1.png&#34; width=&#34;384&#34; /&gt;
&lt;p&gt;We obtained maximum likelihood point estimates and their 95% confidence intervals for each of the three models.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## [1] &amp;#34;f1 parameters&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##  est.a  2.5 % 97.5 % 
##  0.276  0.262  0.289
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## [1] &amp;#34;f2 parameters&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##     est 2.5 % 97.5 %
## a 0.485 0.412  0.576
## h 0.021 0.016  0.026
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## [1] &amp;#34;f3 parameters&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##         est 2.5 % 97.5 %
## a     0.044 0.010  0.161
## h     0.038 0.031  0.043
## theta 1.812 1.385  2.310
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Comparing the performance of the three models using &lt;em&gt;AIC&lt;/em&gt;,
we conclude that there is &amp;ldquo;substantial&amp;rdquo; support for model $f_3$ over the other two models.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;##        AIC   dAIC  df
## fit.f3 538.4   0.0 3 
## fit.f2 552.0  13.6 2 
## fit.f1 613.1  74.7 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Jump back to the &lt;a href=&#34;#overview&#34;&gt;Overview section&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;There is much that we have only touched on and even more we have swept under the rug, but hopefully we&amp;rsquo;re leaving you with a good enough starting point for your own interests.
Most importantly, don&amp;rsquo;t get discouraged or frustrated by the challenge that model fitting can be.
The process of model fitting has often been described as an &amp;ldquo;art&amp;rdquo;.
It is a skill that is learned through practice and accumulated experience, and therefore through perseverance.
And it&amp;rsquo;s worth pointing out that it&amp;rsquo;s a skill to be learned not only in regards to statistical and mathematical knowledge and comfort
(e.g., experience with probability distributions and deterministic functions, respectively),
but also in terms of empirical knowledge of the data and the biological (and data collection) processes that are likely responsible for having generated the data.
Statisticians and mathematicians have just as hard a time thinking about biology as biologists do with the statistics and math!
It&amp;rsquo;s in the challenge of gaining a balanced expertise in all three where the fun and excitement lies.&lt;/p&gt;
&lt;h2 id=&#34;readings&#34;&gt;Readings&lt;/h2&gt;
&lt;p&gt;Aho, Derryberry, and Peterson (2014) Model selection for ecologists: the worldviews of AIC and BIC. Ecology, 95(3):631–636&lt;/p&gt;
&lt;p&gt;Bolker (2008) Ecological models and data in R. Princeton University Press&lt;/p&gt;
&lt;p&gt;Höge, Wöhling, and Nowak (2018) A primer for model selection: The decisive role of model complexity. Water Resources Research, 54(3):1688–1715&lt;/p&gt;
&lt;p&gt;Murtaugh (2014) In defense of p values. Ecology, 95(3):611–617&lt;/p&gt;
&lt;p&gt;Novak and Stouffer (2021) Geometric complexity and the information-theoretic comparison of functional-response models. Frontiers in Ecology and Evolution, 9:776&lt;/p&gt;
&lt;p&gt;Novak and Stouffer (2021) Systematic bias in studies of consumer functional responses. Ecology Letters, 24(3):580–593&lt;/p&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_1.R&#34;&gt;ModelFitting_1.R&lt;/a&gt; (no &amp;lsquo;key&amp;rsquo;)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_2_key.R&#34;&gt;ModelFitting_2_key.R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_3_key.R&#34;&gt;ModelFitting_3_key.R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://models4data2theory.github.io/courses_Rscripts/ModelFitting_4.R&#34;&gt;ModelFitting_4_key.R&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;session-info&#34;&gt;Session Info&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;sessionInfo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## R version 4.4.1 (2024-06-14)
## Platform: aarch64-apple-darwin20
## Running under: macOS Sonoma 14.5
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib 
## LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## time zone: America/Los_Angeles
## tzcode source: internal
## 
## attached base packages:
## [1] stats4    stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
## [1] bbmle_1.0.25.1
## 
## loaded via a namespace (and not attached):
##  [1] cli_3.6.3           knitr_1.48          rlang_1.1.4        
##  [4] xfun_0.46           highr_0.11          jsonlite_1.8.8     
##  [7] htmltools_0.5.8.1   sass_0.4.9          rmarkdown_2.27     
## [10] grid_4.4.1          evaluate_0.24.0     jquerylib_0.1.4    
## [13] MASS_7.3-60.2       fastmap_1.2.0       mvtnorm_1.2-5      
## [16] yaml_2.3.9          lifecycle_1.0.4     numDeriv_2016.8-1.1
## [19] bookdown_0.40       compiler_4.4.1      rstudioapi_0.16.0  
## [22] blogdown_1.19       lattice_0.22-6      digest_0.6.36      
## [25] R6_2.5.1            bslib_0.7.0         Matrix_1.7-0       
## [28] tools_4.4.1         bdsmatrix_1.3-7     cachem_1.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note that you&amp;rsquo;ll more typically see the probability of the binomial written as $P(k |n, p)$ with $n$ treated as a parameter rather than a known variable, but we&amp;rsquo;re going to use $P(k, n|p)$ since $k$ and $n$ both come from data to better contrast probabilities with likelihoods.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Some of you may know (or notice in the lefthand figure) that it&amp;rsquo;s possible for $Pr(k | \lambda) = Pr(k-1 | \lambda )$ (i.e. there is no single maximum to the probability mass function $Pr(k | \lambda)$) &lt;em&gt;when $\lambda$ is an integer&lt;/em&gt;. (See &lt;a href=&#34;https://math.stackexchange.com/questions/4144800/poisson-distribution-with-an-integer-lambda-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this StackExchange post&lt;/a&gt; for an explanation.)
However, this is &lt;em&gt;not&lt;/em&gt; the case for the likelihood function (i.e. there is a single maximum) when we plot the likelihood as a function of the parameter (as shown in the righthand figure).&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;By convention we use the natural logarithm, $\ln = \log_{e}$.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For simple models, the likelihood will typically only have one minimum and no maximum (or maxima) to worry about.  But we&amp;rsquo;ll come back to this below.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;code&gt;optim()&lt;/code&gt; will work in one-dimensional (single-parameter) situations, as we are working with at present, but the default &lt;code&gt;Nelder–Mead&lt;/code&gt; method that works very well in most higher-dimensional situations often does not work well for one-dimensional situations and thus issues a warning.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;We here focus on theoretical models that generate predictions for the &lt;em&gt;expected&lt;/em&gt; value (the mean) of the response variable as a function of the covariate(s).
These are by far the most common type of theoretical models. However, there are models that generate predictions for the variance, etc. as well, which can be just as useful in fitting models to data through the choice of appropriate probability distributions (i.e. likelihood functions).
As noted above, the mean and variance of the Poisson are in fact the same, and they are entirely dependent for the binomial, so we are making &amp;mdash; and making use of &amp;mdash; that assumption when we&amp;rsquo;re fitting out data with them.&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;hellip;when visitation rates are expressed on a &lt;em&gt;per pollinator&lt;/em&gt; basis!   For our exercise, we can think of having arbitrarily set the pollinator abundance to $A=1$ for all visitation counts. In an empirical dataset the total number of visits would be $f(P) \cdot A$ and we&amp;rsquo;d have to supply the vector of pollinator abundances to our likelihood function as well.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The model is also called the &lt;em&gt;generalized Holling Type III&lt;/em&gt;.  The original two-parameter &lt;em&gt;Holling Type III&lt;/em&gt; has $\theta = 2$ corresponding to a search rate that increases linearly with $P$.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Recall that, for simplicity, we&amp;rsquo;re here assuming that the number of pollinators is constant across all our observations.&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This touches on issues central to an active discussion between and within the worlds of statistics and machine learning.&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Also note that the literature often uses $k$ to represent the count of the parameters.  We&amp;rsquo;ll use $p$ here to avoid confusion with our prior usage of $k$ as the count of pollinator visits.&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The issues are similar to those involved in how to evaluate &amp;ldquo;significance&amp;rdquo; using p-values.&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For linear models this cutoff has been shown to correspond closely to considering a $p &amp;lt; 0.05$ to be statistically &amp;ldquo;significant&amp;rdquo;.&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>Of Potential Interest</title>
      <link>https://models4data2theory.github.io/courses/potential_use_or_interest/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://models4data2theory.github.io/courses/potential_use_or_interest/</guid>
      <description>&lt;p&gt;$$
\newcommand{\L}{\mathcal{L}}
$$&lt;/p&gt;
&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability distributions&lt;/h2&gt;
&lt;h4 id=&#34;Prob_mass_dens&#34;&gt;Mass vs. density&lt;/h4&gt;
&lt;p&gt;For discrete probability distributions,
$P(O | \theta)$ is typically referred to as a &lt;em&gt;probability mass function&lt;/em&gt;.
Given the parameter(s) $\theta$, each integer value of an outcome $O$
(i.e. the discrete random variable) has some probability (some &amp;ldquo;mass&amp;rdquo;) of occurring.&lt;/p&gt;
&lt;p&gt;In contrast, for continuous probability distributions,
we instead refer to &lt;em&gt;probability density functions&lt;/em&gt;, $f(O | \theta)$.
That&amp;rsquo;s because the probability of any specific value of a continuous random variable is zero!
Instead, there is only a non-zero probability of the random variable falling &lt;em&gt;within some interval&lt;/em&gt; of values.
The higher the probability density over that interval,
the higher the probability of observing a measurable value from within it.&lt;/p&gt;
&lt;h4 id=&#34;likelihood-functions&#34;&gt;Likelihood functions&lt;/h4&gt;
&lt;p&gt;The distinction between probability mass functions and probability density functions is relevant to the likelihood functions for discrete and continuous random variables.
For discrete variables (as we will focus on in class), the likelihood of the parameter(s) given the data is equal to the probability of the data given the parameter(s),
$$
L(\theta | O) = P(O | \theta).
$$
For continuous variables, we maximize the likelihood of the parameter given the data by finding the parameter that maximizes the probability density function.
That is, we maximize
$$
L(\theta | O) = f(O | \theta).
$$&lt;/p&gt;
&lt;h2 id=&#34;max_lik_math_poisson&#34;&gt;Analytical MLE for Poisson&lt;/h2&gt;
&lt;p&gt;Given $n$ observations (counts) from a process presumed to be well-described by the Poisson, we have that
$$
-\ln \L(\lambda |k)
= -\sum_i^n \ln \left (\frac{\lambda^{k_i} e^{-\lambda}}{k_i!} \right) .
$$
Remembering that logarithms transform multiplicative processes into additive processes,
we can write this as
$$
-\ln \L(\lambda |k)
= -\sum_i^n \left ( \ln ( \lambda^{k_i} ) + \ln (e^{-\lambda} ) - \ln (k_i!)  \right).
$$
Since $\ln x^y = y \ln x$ and $\ln e^x = x$, we can simplify this to
$$
-\ln \L(\lambda |k)  = -\sum_i^n \left ( k_i \ln (\lambda) -\lambda - \ln (k_i!)  \right).
$$
Distributing the summation, we get
$$
-\ln \L(\lambda |k)  = -\sum_i^n \left ( k_i \ln \lambda \right)  + n\lambda + \sum_i^n  \ln (k_i!).
$$
Now take the derivative with respect to $\lambda$, set it equal to zero, and solve for $\lambda$:
$$
\frac{d -\ln \L(\lambda | k)}{d \lambda} = - \frac{1}{\lambda} \sum_i^n k_i + n = 0 \implies \lambda = \frac{\sum_i^n k_i}{n},
$$
which is the mean value of all the $k_i$ counts!&lt;/p&gt;
&lt;p&gt;Our analytically-solved maximum likelihood estimator for $\lambda$, which we will symbolize by $\hat{\lambda}$, is therefore
$$
\hat{\lambda} = \frac{\sum_i^n k_i}{n}.
$$
This is nothing more than a function, think $\hat{\lambda}(k,n)$, to which we provide a vector of observed $k$ counts and their $n$ number (the vector&amp;rsquo;s length) as inputs.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
